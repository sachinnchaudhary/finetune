# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ccLm4QIM8XhphFVYntXoGbH9E4tf-QRJ
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip -q install --upgrade "transformers" "accelerate" "peft" "bitsandbytes" "datasets" "huggingface_hub" "sentencepiece"
!pip install trl

from huggingface_hub import login
token = "Your huggingface Token"

import torch, os
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
)
from peft import LoraConfig, prepare_model_for_kbit_training
from trl import SFTTrainer

MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"

is_bf16 = torch.cuda.is_bf16_supported()
compute_dtype = torch.bfloat16 if is_bf16 else torch.float16

attn_impl = "sdpa" if torch.__version__ >= "2.1" else "eager"
if "T4" in torch.cuda.get_device_name(0):
    attn_impl = "eager"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=compute_dtype,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, token = auth)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

import bitsandbytes
print(bitsandbytes.__version__)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_config,
    attn_implementation=attn_impl,
    torch_dtype=compute_dtype,
    device_map="auto",
)

model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

ds = load_dataset("databricks/databricks-dolly-15k", split="train")

ds = ds.shuffle(seed=42).select(range(2000))

def to_chat(example):
    instr = example["instruction"]
    ctx   = example.get("context") or ""
    resp  = example["response"]
    user  = instr if not ctx else (instr + "\n\n" + ctx)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user",   "content": user},
        {"role": "assistant","content": resp},
    ]
    example["text"] = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=False
    )
    return example


ds = ds.map(to_chat, remove_columns=ds.column_names)

peft_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    target_modules=["q_proj","k_proj","v_proj","o_proj"],
    task_type="CAUSAL_LM",
)

sft_config = TrainingArguments(
    output_dir="qwen25_1p5b_dolly_qlora",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,      # effective batch = 8
    num_train_epochs=1,                 # or set max_steps for precise budget
    learning_rate=2e-4,                 # QLoRA-typical LR
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=10,
    save_strategy="steps",
    save_steps=100,
    optim="paged_adamw_8bit",           # bitsandbytes optimizer
    bf16=is_bf16,
    fp16=not is_bf16,
    gradient_checkpointing=True,
    max_grad_norm=0.3,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,
    peft_config=peft_cfg,
    train_dataset=ds,
    args= sft_config,
)

trainer.model.print_trainable_parameters()  # sanity: should be ~0.1–1% of total
trainer.train()

from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer, pipeline
import torch, os

# Path to your locally saved adapter folder (note the "./")
adapter_path = "./qwen25_1p5b_dolly_qlora_adapter"

# Load the base model first, then apply the adapter
base_model = "Qwen/Qwen2.5-1.5B-Instruct"

model = AutoPeftModelForCausalLM.from_pretrained(
    adapter_path,
    device_map="auto",
    torch_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    device_map="auto"
)

prompt = "Explain the difference between LoRA and QLoRA to a new grad."
messages = [
    {"role": "system", "content": "You are a concise, rigorous mentor."},
    {"role": "user", "content": prompt},
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
out = pipe(text, max_new_tokens=200, do_sample=True, temperature=0.7)
print(out[0]["generated_text"][len(text):])

!ls qwen25_1p5b_dolly_qlora_adapter

# Save only the adapters (small!) – quickest for sharing and inference
trainer.model.save_pretrained("qwen25_1p5b_dolly_qlora_adapter")
tokenizer.save_pretrained("qwen25_1p5b_dolly_qlora_adapter")

# Optional: merge LoRA into full fp16 weights (use CPU to avoid OOM)
from peft import PeftModel
base = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=None)
merged = PeftModel.from_pretrained(base, "qwen25_1p5b_dolly_qlora_adapter").merge_and_unload()
merged.save_pretrained("qwen25_1p5b_dolly_qlora_merged", safe_serialization=True)

